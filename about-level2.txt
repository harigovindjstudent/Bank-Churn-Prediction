At level two, things get more interesting and a little more 
challenging. You're now working with messier, more realistic data 
and structuring your projects like a professional data scientist 
rather than just messy experiments and notebooks.
Your tools and workflow have evolved in the following ways. You're 
moving from a single notebook to a well organized Python project with 
separate modules for data processing, feature engineering, model training,
and evaluation. You use Git for version control, and you're creating 
configuration files to keep experiments reproducible. Instead of just 
random shuffling, you're using proper train validation test splits, 
often with things like walk forward validation for time series data. 
You're tackling issues like class imbalance using techniques like 
smokeote or adjusting class weights and applying modern feature 
engineering tools. You might be using more interesting models like 
light GBM, simple neural networks, or even AI APIs. You're thinking 
about hyperparameter tuning and maybe even experimenting with more 
advanced options like Beijian search. And perhaps you're making a 
simple pipeline with tools like Prefect. Imagine a typical level two 
project. This could be something like building a customer churn 
prediction pipeline that uses data from multiple sources like 
transaction records, support interactions and usage logs.
Handling imbalanced classes and performing feature selection to 
identify the most predictive variables and evaluating your model 
using precision recall curves, rock curves, and business 
specific metrics. This is the stage where your work becomes more 
structured and robust.
